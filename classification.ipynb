{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7015ae-1b9a-41ec-ad50-8a4bba1247d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abk171/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pydicom\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b88e39-a32e-47d2-ab64-302a3711d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/data_vault/hexai02/CarpalTunnel/AI Project Data Clean 10-8.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7eb460-1c97-49ab-af88-b8e3ebc4358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>WristImage Segmentation for AI Analysis</th>\n",
       "      <th>Arm (L/R)</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Age</th>\n",
       "      <th>Clinical Signs of CTS</th>\n",
       "      <th>CTS-6 Score</th>\n",
       "      <th>Measurement at Wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>Yes</td>\n",
       "      <td>R</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>12.5</td>\n",
       "      <td>12.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>Yes</td>\n",
       "      <td>R</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Unreported/Chose not to disclose</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>16.5</td>\n",
       "      <td>20.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>Yes</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Unreported/Chose not to disclose</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>11.5</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>Yes</td>\n",
       "      <td>R</td>\n",
       "      <td>F</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>63.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>Yes</td>\n",
       "      <td>R</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>163</td>\n",
       "      <td>Yes</td>\n",
       "      <td>R</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>74.0</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>164</td>\n",
       "      <td>Yes</td>\n",
       "      <td>R</td>\n",
       "      <td>F</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>67.0</td>\n",
       "      <td>N</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>165</td>\n",
       "      <td>yes</td>\n",
       "      <td>R</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>65.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>166</td>\n",
       "      <td>Yes</td>\n",
       "      <td>L</td>\n",
       "      <td>F</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>16.5</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>167</td>\n",
       "      <td>Yes</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>21.5</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number WristImage Segmentation for AI Analysis Arm (L/R) Sex   Race  \\\n",
       "0      21                                     Yes         R   M  White   \n",
       "1      22                                     Yes         R   M  White   \n",
       "2      23                                     Yes         L   M  White   \n",
       "3      24                                     Yes         R   F  Black   \n",
       "4      25                                     Yes         R   M  White   \n",
       "..    ...                                     ...       ...  ..    ...   \n",
       "95    163                                     Yes         R   F  White   \n",
       "96    164                                     Yes         R   F  Black   \n",
       "97    165                                     yes         R   F  White   \n",
       "98    166                                     Yes         L   F  White   \n",
       "99    167                                     Yes         ?   F  Black   \n",
       "\n",
       "                           Ethnicity   Age Clinical Signs of CTS  CTS-6 Score  \\\n",
       "0                       Non-Hispanic  43.0                     Y         12.5   \n",
       "1   Unreported/Chose not to disclose  74.0                     Y         16.5   \n",
       "2   Unreported/Chose not to disclose  74.0                     Y         11.5   \n",
       "3                       Non-Hispanic  63.0                     Y         21.0   \n",
       "4                       Non-Hispanic  49.0                     Y          9.0   \n",
       "..                               ...   ...                   ...          ...   \n",
       "95                      Non-Hispanic  74.0                     ?          NaN   \n",
       "96                      Non-Hispanic  67.0                     N          3.5   \n",
       "97                      Non-Hispanic  65.0                     N          0.0   \n",
       "98                      Non-Hispanic  51.0                     Y         16.5   \n",
       "99                      Non-Hispanic  32.0                   Yes         21.5   \n",
       "\n",
       "    Measurement at Wrist  \n",
       "0                  12.16  \n",
       "1                  20.10  \n",
       "2                  13.83  \n",
       "3                  16.55  \n",
       "4                  19.21  \n",
       "..                   ...  \n",
       "95                 12.78  \n",
       "96                  7.57  \n",
       "97                 16.10  \n",
       "98                  9.95  \n",
       "99                 18.68  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b078b722-134c-46a7-b4e1-1182a5dd3712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Y', 'N', 'Yes', '?'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Clinical Signs of CTS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32e8e30-3e6b-4b87-83de-83de4725d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clinical Signs of CTS'] = df['Clinical Signs of CTS'].replace('Yes', 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5e6712-9e67-42ed-a748-8d7f46e7d25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Y', 'N', '?'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Clinical Signs of CTS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9024687f-1deb-4f82-9782-5619ea95192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "count_question_mark = (df['Clinical Signs of CTS'] == '?').sum()\n",
    "print(count_question_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a599f82-84d4-4e7f-ac8a-ef4c503624d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Clinical Signs of CTS'] != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b222ab7-2511-42aa-8398-7df1d3f85cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number'] = df['Number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e691e5b8-2e42-4273-a494-40195664a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructured_root = 'data_yolo'\n",
    "image_dir = os.path.join(restructured_root, 'images')\n",
    "image_train_dir = os.path.join(image_dir, 'train')\n",
    "image_val_dir = os.path.join(image_dir, 'val')\n",
    "image_test_dir = os.path.join(image_dir, 'test')\n",
    "label_dir = os.path.join(restructured_root, 'labels')\n",
    "label_train_dir = os.path.join(label_dir, 'train')\n",
    "label_val_dir = os.path.join(label_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa7d14bc-efeb-401f-bea1-4ab3803210ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dir = '/data_vault/hexai02/CarpalTunnel/Annotations'\n",
    "dicom_dir = '/data_vault/hexai02/CarpalTunnel/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8210f6f1-14f2-41fd-b46e-e2c6e95d13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(mask_path):\n",
    "    mask = sitk.ReadImage(mask_path)\n",
    "    return sitk.GetArrayFromImage(mask)[0][: 450, 200: 1300]\n",
    "\n",
    "def load_dicom(dicom_path):\n",
    "    dicom_data = pydicom.dcmread(dicom_path)\n",
    "    return dicom_data.pixel_array[: 450, 200: 1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f68257-e89d-4361-be18-e3a2b90e4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"runs/detect/train17/weights/best.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5cc5c38-9a06-46a5-95e3-8b03975c9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epochs': 40,\n",
    "    'in_channels': 1,\n",
    "    'dropout': 0.2,\n",
    "    'decoder_attention_type': 'scse',\n",
    "    'init_lr': 5e-4,\n",
    "    'weight_decay': 0.05,\n",
    "    'T_max': 10,\n",
    "    'eta_min': 3e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22743bf7-dfb0-400d-b99a-7f1835254733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        optimizer = None\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    else:\n",
    "        scheduler = None\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa2346-388a-4fce-bd14-9b699b9263ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "unet_model = smp.Unet(encoder_name=\"resnet18\",in_channels=1,dropout=config['in_channels'],decoder_attention_type=config['decoder_attention_type']).to(device)\n",
    "\n",
    "# unet_model, _, _, _ = load_model(unet_model, optimizer=None, scheduler=None, path=f'unet_runs/epoch_40.pth')\n",
    "checkpoint = torch.load('unet_runs/epoch_40.pth', map_location=\"cuda\")\n",
    "unet_model.load_state_dict(checkpoint, strict=True)\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f54354d-3e4e-43bf-a766-672458c23b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Number', 'WristImage Segmentation for AI Analysis', 'Arm (L/R)', 'Sex',\n",
       "       'Race', 'Ethnicity', 'Age', 'Clinical Signs of CTS', 'CTS-6 Score',\n",
       "       'Measurement at Wrist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77fd73ef-769c-4671-8182-20c293b34c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/87 [00:08<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:09<00:00,  8.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:03<00:00, 10.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "\n",
    "image_transform = T.Compose([\n",
    "    T.Resize((512, 512), interpolation=T.InterpolationMode.BILINEAR),  # âœ… Resize first\n",
    "    T.Normalize(mean=[0.5], std=[0.5]),  # âœ… Normalize after\n",
    "])\n",
    "\n",
    "def fill_arr(img_dir, img_bucket, mask_bucket):\n",
    "    for jpg_name in tqdm(os.listdir(img_dir)): \n",
    "        entry_name = jpg_name.split('.')[0]\n",
    "        img_filename = entry_name + '.dcm'\n",
    "\n",
    "        # df['Clinical Signs of CTS'].unique()\n",
    "        clinical_signs = df.loc[df['Number'] == entry_name, 'Clinical Signs of CTS'].values\n",
    "        if len(clinical_signs) == 0:\n",
    "            print(f\"{entry_name}\")\n",
    "            continue\n",
    "        classification_tensor = torch.tensor([1 if clinical_signs[0] == 'Y' else 0], dtype=torch.float32)\n",
    "        train_labels.append(classification_tensor)\n",
    "        # Load DICOM image\n",
    "        img = load_dicom(os.path.join(dicom_dir, img_filename))\n",
    "\n",
    "        # Get bounding box from YOLO\n",
    "        result = model(img, save=False, verbose=False)\n",
    "        box = result[0].boxes\n",
    "        \n",
    "        if len(box.xyxy.tolist()) == 0:\n",
    "            continue  \n",
    "\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy.tolist()[0])\n",
    "        img_slice = img[y1:y2, x1:x2, 0]  # Crop the region\n",
    "\n",
    "\n",
    "        img_tensor = torch.from_numpy(img_slice).float()  # Convert to float\n",
    "\n",
    "\n",
    "        img_tensor = img_tensor / 255.0  \n",
    "\n",
    "\n",
    "        if img_tensor.dim() == 2:\n",
    "            img_tensor = img_tensor.unsqueeze(0).unsqueeze(0)  # Convert (H, W) â†’ (1, H, W)\n",
    "\n",
    "\n",
    "        img_tensor = image_transform(img_tensor)  \n",
    "        \n",
    "\n",
    "        img_tensor = img_tensor.to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask_gpu = unet_model(img_tensor)  \n",
    "        \n",
    "\n",
    "        img_resized = img_tensor.squeeze().cpu().detach().numpy()  \n",
    "        mask = mask_gpu.squeeze().cpu().detach().numpy()  \n",
    "\n",
    "\n",
    "        img_2channel = np.stack((img_resized, mask), axis=0)\n",
    "        img_bucket.append(img_2channel)\n",
    "\n",
    "        \n",
    "\n",
    "fill_arr(image_train_dir, train_images, train_labels)\n",
    "fill_arr(image_val_dir, val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2ece772-430f-4526-9666-d883371cfedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 512)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c988e8cf-34d1-47d6-927f-a2bd71740c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # List of image arrays\n",
    "        self.labels = labels  # List of label tensors\n",
    "        self.transform = transform  # Augmentations\n",
    "    \n",
    "    def threshold_mask(self, mask):\n",
    "        \"\"\"Binarizes mask: pixel > 0.5 â†’ 1, else 0\"\"\"\n",
    "        return torch.where(mask > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
    "    \n",
    "    def apply_random_cutout(self, image):\n",
    "        \"\"\"Applies a small random cutout (1-5% of image size)\"\"\"\n",
    "        c, h, w = image.shape\n",
    "        cutout_size = random.randint(int(0.01 * h * w), int(0.05 * h * w))  # 1-5% area\n",
    "        cut_h = int(np.sqrt(cutout_size))\n",
    "        cut_w = cut_h\n",
    "\n",
    "        x = random.randint(0, w - cut_w)\n",
    "        y = random.randint(0, h - cut_h)\n",
    "\n",
    "        image[:, y:y+cut_h, x:x+cut_w] = 0  # Black-out region\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Still an array\n",
    "        label = self.labels[idx]  # Already a tensor\n",
    "\n",
    "        # Convert image to tensor if it's not already\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Ensure correct shape (C, H, W)\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.unsqueeze(0)  # Convert grayscale image to (1, H, W)\n",
    "\n",
    "        # Apply mask thresholding (only to the 2nd channel)\n",
    "        image[1] = self.threshold_mask(image[1])\n",
    "\n",
    "        # Apply random cutout\n",
    "        # image = self.apply_random_cutout(image)\n",
    "\n",
    "        # Apply optional transforms (if provided)\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "\n",
    "        return image, label  # Label remains a tensor\n",
    "\n",
    "# Augmentations for classification\n",
    "transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.GaussianBlur(kernel_size=3),\n",
    "])\n",
    "\n",
    "# Creating Dataset\n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n",
    "val_dataset = CustomDataset(val_images, val_labels, transform=None)  # No augmentation for validation\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e46c42f-ba01-449a-8d03-15ee8587a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 512, 512]) torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_loader:\n",
    "    print(image.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da317c-efb6-451b-b64a-7047e5aded95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(models.convnext_tiny(input_channels=2, num_classes=2), (3,224,224), device=\"cpu\") # 96\n",
    "# summary(models.resnet34(), (3,224,224), device=\"cpu\") # 64\n",
    "# summary(models.mobilenet_v3_small(), (3,224,224), device=\"cpu\") # 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ecc6df-625b-4a96-b78b-c578e065d985",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ed997ce-d128-4573-a2d9-0b563bc4210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=6, stride=1, **kwargs):\n",
    "        super(MobileNetV2Block, self).__init__()\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expansion_channels = in_channels * expansion_factor\n",
    "        \n",
    "        self.c1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=self.expansion_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.act1 = nn.ReLU6()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(self.expansion_channels)\n",
    "\n",
    "        self.d1 = nn.Conv2d(\n",
    "            in_channels=self.expansion_channels,\n",
    "            out_channels=self.expansion_channels,\n",
    "            kernel_size=3,\n",
    "            stride=self.stride,\n",
    "            padding=1,\n",
    "            groups=self.expansion_channels,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.act2 = nn.ReLU6()\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(self.expansion_channels)\n",
    "\n",
    "        self.c2= nn.Conv2d(\n",
    "            in_channels=self.expansion_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.residual = (self.stride == 1 and self.in_channels == self.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        x = self.c1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.residual:\n",
    "            x += out\n",
    "\n",
    "        return x                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d20ab051-b373-49d6-be2b-b71d0671e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2Layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n, **kwargs):\n",
    "        super(MobileNetV2Layer, self).__init__()\n",
    "        layers = [MobileNetV2Block(in_channels, out_channels, **kwargs)] + [MobileNetV2Block(out_channels, out_channels, **kwargs) for _ in range(n-1)]\n",
    "\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53edec6d-695f-49b0-901e-d4eb12dcf4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, in_channels=3, \n",
    "                 classes=1000, \n",
    "                 expansion_factor_list = [-1,1,6,6,6,6,6,6], \n",
    "                 out_channel_list = [32,16,24,32,64,96,160,320,1280], \n",
    "                 layer_list = [1,1,2,3,4,3,3,1], \n",
    "                 stride_list = [2,1,2,2,2,1,2,1]):\n",
    "        \n",
    "        super(MobileNetV2, self).__init__()\n",
    "        \n",
    "        self.c1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channel_list[0],\n",
    "            kernel_size=3,\n",
    "            stride=stride_list[0],\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel_list[0])\n",
    "        self.act1 = nn.ReLU6()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(1, len(expansion_factor_list)):\n",
    "            expansion_factor = expansion_factor_list[i]\n",
    "            in_channels = out_channel_list[i-1]\n",
    "            out_channels = out_channel_list[i]\n",
    "            n = layer_list[i]\n",
    "            stride = stride_list[i]\n",
    "\n",
    "            layers.append(MobileNetV2Layer(in_channels, \n",
    "                                           out_channels, \n",
    "                                           n,\n",
    "                                           expansion_factor=expansion_factor,\n",
    "                                           stride=stride))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.c2 = nn.Conv2d(\n",
    "            in_channels=out_channel_list[-2],\n",
    "            out_channels=out_channel_list[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel_list[-1])\n",
    "        self.act2 = nn.ReLU6()\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # self.c3 = nn.Conv2d(\n",
    "        #     in_channels=out_channel_list[-1],\n",
    "        #     out_channels=classes,\n",
    "        #     kernel_size=1,\n",
    "        #     bias=False\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.gap(x)\n",
    "\n",
    "        # x = self.c3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5053d-8357-45b5-9ca7-ee4f3e723ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(MobileNetV2(),(3,224,224),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25406513-07cf-4526-b4ea-47f60e390cb2",
   "metadata": {},
   "source": [
    "### ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9005a47e-739c-422a-8247-ee5bb3e5427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLayerNorm(nn.Module):\n",
    "  def __init__(self, dims, eps=1e-6):\n",
    "    super().__init__()\n",
    "    self.norm = nn.LayerNorm(dims, eps)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = self.norm(x)\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93f634a8-14ee-4e07-ae6b-4877e319dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock2D(nn.Module):\n",
    "  def __init__(self, dim, layer_scale_init_value=1e-6, drop=0.2):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "\n",
    "    self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "    self.act = nn.GELU()\n",
    "    self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "\n",
    "    self.dropout = nn.Dropout2d(p=drop)\n",
    "\n",
    "    self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)), requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    x = self.dwconv(x)\n",
    "\n",
    "    # Transpose for LayerNorm\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = self.norm(x)\n",
    "\n",
    "    x = self.pwconv1(x)\n",
    "    x = self.act(x)\n",
    "    x = self.pwconv2(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    if self.gamma is not None:\n",
    "        x = self.gamma * x\n",
    "\n",
    "    # Transpose back to (B, C, H, W)\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    # no drop path y\n",
    "    return residual + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19cfe9-7fd7-4f7a-8802-b97674a20767",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(ConvNeXtBlock2D(96), (96,112,112), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63a43e34-9aa4-47e8-b30a-a3524c149c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNext(nn.Module):\n",
    "    def __init__(self, in_chans=1, dims=[32, 64, 128, 256], stages=[1, 1, 3, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.dims = dims\n",
    "        self.stages = stages\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            myLayerNorm(dims[0], eps=1e-6)\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                myLayerNorm(dims[i], eps=1e-6),\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.model_layers = nn.ModuleList()\n",
    "        for i, stage_length in enumerate(stages):\n",
    "            stage = nn.ModuleList([ConvNeXtBlock2D(dims[i]) for _ in range(stage_length)])\n",
    "            self.model_layers.append(stage)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.final_norm = nn.LayerNorm(dims[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.dims)):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            for layer in self.model_layers[i]:\n",
    "                x = layer(x)\n",
    "\n",
    "        x = self.pooling(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.final_norm(x)  # Final normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76992125-21ba-4b93-a9df-dd511f93fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(ConvNext(in_chans=2,dims=[96,192,384,768],stages=[1,1,3,1]), (2,512,512), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce8e5534-a9ee-4ce6-a7ba-716db26626df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTail(nn.Module):\n",
    "    def __init__(self, in_features, num_classes=2, dropout_rate=0.5):\n",
    "        super(ClassificationTail, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features // 2),\n",
    "            nn.BatchNorm1d(in_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(in_features // 2, in_features // 4),\n",
    "            nn.BatchNorm1d(in_features // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(in_features // 4, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "603b9c31-5e41-4682-9418-656f108629e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, backbone, tail):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.tail = tail\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533809b0-2be6-4e20-b5fd-3187fa46043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(FullModel(ConvNext(in_chans=2,dims=[96,192,384,768],stages=[1,1,3,1]), \n",
    "                  ClassificationTail(768,num_classes=2,dropout_rate=0.2)), (2,512,512),device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
